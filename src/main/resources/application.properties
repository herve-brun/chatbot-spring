spring.application.name=demo
# Ollama
langchain4j.ollama.streaming-chat-model.base-url=http://localhost:11434
langchain4j.ollama.streaming-chat-model.model-name=gemma3:27b
langchain4j.ollama.streaming-chat-model.timeout=PT10S
langchain4j.ollama.streaming-chat-model.max-retries=1
langchain4j.ollama.streaming-chat-model.log-requests=true
langchain4j.ollama.streaming-chat-model.log-responses=true

logging.level.dev.langchain4j=DEBUG

management.endpoints.web.exposure.include=*

management.tracing.sampling.probability=1
management.endpoint.shutdown.access=unrestricted

spring.boot.admin.client.url=http://localhost:8080
management.info.env.enabled=true